\documentclass[14pt]{article}

\begin{document}

\title{Review on Practical Graph Mining with R}
\maketitle

\section{Chapter 1 on May 18, 2015}
\begin{enumerate}
 \item A graph is a collection of individual objects interconnected in some way. Graph data analytics refer to the extraction of insightful and actionable knowledge from graph data.
 
 \item Applications of graph mining 
  \begin{itemize}
   \item Web graphs, e.g.., page ranks.
   \item Social science graphs, which models the relationships among individuals and organizations. Links represent friendship, political alliance, professional collaboration, etc.
   \item Computer networking graphs represent interconnections among various routers across the Internet. 
   \item Homeland security and cybersecurity graphs
   \item Biological graphs
   \item Chemical graphs
   \item Finance graphs. The structure of stock markets and trading records can be represented using graphs, e.g., nodes are brokers, banks, and customers; links capture the financial trading information. A sequence of such graphs over a period of time can be mined to detect people involved in financial frauds, to predict which stocks will be on the rise, and to distinguish stock purchasing patterns that may lead to profits or losses.
   \item Healthcare graphs
  \end{itemize}
  
  \item Dimensionality reduction in general refers to the problem of reducing the data amount while preserving the essential or more salient characteristics of the data. In terms of graph, dimensionality reduction refers to the problem of transforming graphs into low-dimensional vectors so that graph features and similarities are preserved. 
\end{enumerate}

\section{Chapter 2 on May 18, 2015}
\begin{enumerate}
 \item A graph is a theoretical construct composed of points (i.e., vertices) connected by lines (i.e., edges). Graphs are structural data. The vertices of a graph symbolize discrete pieces of information, while the edges of a graph symbolize the relationships between those pieces. 
 
 \item A vertex is also called a node. It is a single point in a graph. Vertices are usually labeled. An edge can be regarded as a line connecting two vertices. Edges may have labels as well. Vertices and edges are the basic building blocks of graphs.
 \begin{itemize}
  \item A graph $G$ is composed of two sets, i.e., a set of vertices, denoted as $V(G)$; and a set of edges, denoted as $E(G)$.
  \item An edge in a graph $G$ is an unordered pair of two vertices $(v_1,v_2)$ such that $v_1 \in V(G)$ and $v_2 \in V(G)$.
  \item An edge is said to join its two vertices. Likewise, two vertices are said to be adjacent if and only if there is an edge between them. Two vertices are said to be connected if there is a path between one to the other via any number of edges.
  \item A loop is an edge that joins a vertex to itself.
  \item An edge is a multiple edge if there is another edge in $E(G)$ which joins the same pair of vertices.
  \item Note that multiple edges and loops often make manipulating graphs more difficult. Many proofs and algorithms in graph theory require them to be excluded.
  \item A \emph{simple} graph is a graph with no loops or multiple edges.
  \item An edge always has exactly two vertices, but a single vertex can be an endpoint for zero, one, or many edges. The \emph{degree} of a vertex $v$, denoted as $degree(v)$, is the number of times $v$ occurs as an endpoint for the edges $E(G)$. It indicates that the dgree of a vertex is the number of edges leading to it. Note that a loop adds 2 to the degree of a vertex.
 \end{itemize}
 
 \item A subgraph $S$ of a graph $G$ is a set of vertices.
  \begin{itemize}
   \item A set of vertices $V(S) \subset V(G)$
   \item A set of edges $E(S) \subset E(G)$. Every edge in $E(S)$ must be an unordered pair of vertices $(v_1, v_2)$ such that $v_1 \in V(S)$ and $v_2 \in V(S)$. It indicates that an edge can only be part of a subgraph if both its endpoints are part of the subgraph.
  \end{itemize}

\item A subgraph can be induced (i.e., induced graph) in two ways, i.e., by vertices and edges.

\item Two graphs $G$ and $H$ are \emph{isomorphic}, denoted as $G \simeq H$, if there exists a bijection $f: V(G) \rightarrow V(H)$ such that an edge $(v_1, v_2) \in E(G)$ if and only if $(f(v_1), f(v_2)) \in E(H)$. Informally, it indicates that two graphs are isomorphic if they can be drawn in the same shape. If $G$ and $H$ are isomorphic, the bijection $f$ is said to be an isomorphism between $G$ and $H$ and between $H$ and $G$. The isomorphism class of $G$ is all graphs isomorphic to $G$. 

\item Note that when vertices and edges have labels, the notion of \emph{sameness} and \emph{isomorphism} are different. Two graphs having the same structure and thus \emph{isomorphic}, but have different labels, and are thus not exactly the same. Labeled graphs are i\emph{isomorphic} if their underlying unlabeled graphs are \emph{isomorphic}. 

\item An automorphism between graphs $G$ and $H$ is an isomorphism $f$ that maps $G$ onto iteself. The automorphism class of G is all graphs automorphic to G. It indicates that the graph structures are the same and the labels are the same too. All automorphisms are isomorphisms, but not all isomorphisms are automorphisms. 

\item One common problem that is often encountered in graph mining is the subgraph isomorphism problem. The subgraph isomorphism problem asks if, given two graphs $G$ and $H$, does $G$ contain a subgraph isomorphic to $H$. That is, given a larger graph $G$ and a smaller graph $H$, whether there is a subgraph in $G$ that is the same shape as $H$. The problem is NP-complete, meaning it is computationally expensive.

\item A directed graph or digraph $D$ is composed of two sets
 \begin{itemize}
  \item A set of vertices $V(D)$
  \item A set of edges $E(D)$, such that each edge is an ordered pair of vertices $(t,h)$. The first vertex $t$ is called the tail, while the latter vertex $h$ is called the head. The edge in a directed graph is usually drawn as an arrow with the arrow-head pointing towards the head vertex. 
  \item Indegree of a vertex $v$ is the number of edges in $E(D)$ which have $v$ as the head.
  \item Outdegree of a vertex $v$ is the number of edges in $E(D)$ which have $v$ as the tail.
  \item Digraph isomorphism specifies that two digraphs $J$ and $K$ are isomorphic if and only if their underlying undirected graphs are isomorphic. So similar to labeled graphs, the direction of edges are not considered when considering isomorphism. Two digraphs are isomorphic if you can change all the directed edges to undirected edges and then draw them in the same shape. 
 \end{itemize}
 
 \item Families of graphs
 \begin{itemize}
  \item A clique is a set of vertices which are all connected to each other by edges. A set of vertices $C$ is a clique in the graph $G$ if for all pairs of vertices $v_1 \in C$ and $v_2 \in C$, there exists an edge $(v_1, v_2) \in E(G)$. If begin at one vertex in a clique, you can get to any other member of that clique by following only one edge.
  \item A complete graph with $n$ vertices, denoted as $K_n$, is a graph such that $V(K_n) $ is a clique.
  \item A path of length $n$, denoted as $P_n$, in a graph $G$ is an ordered set of edges ${(v_0,v_1),(v_1,v_2),(v_3,v_4), \cdots, (v_{n-1},v_n)}$ such that each edge $e \in E(G)$. A path is sometimes called a walk. Note that there may be more than one path between the same two vertices in a graph. 
  \item A path is closed if its first and last vertices are the same. A cycle of length $n$, denoted as $C_n$, in a graph $G$ is a closed path of length $n$. Note that a simple cycle is the same as a simple closed path, with the exception that it may visit one vertex exactly twice, i.e., the vertex which is both the start and end of the cycle.
  \item Trees are graphs that obey certain structural rules and have many appealing mathematical properties. A tree graph has exactly one vertex as the root, i.e., parent. It can have any number of children vertices adjacent to it. Those children can in turn, be parent of their own children vertices. A vertex having no children is called a leaf. 
   \begin{itemize}
    \item A graph $G$ is a tree if and only if there is exactly one simple path from each vertex to every other vertex. Similarly, it can be defined as a graph $G$ if and only if it is a connect graph with no cycles. Trees are often modeled as directed graphs. Hence, the root has an indegree of 0. All the other vertices have indegree of exactly 1. Leaf vertices have an outdegree of 0.
   \end{itemize}
 \end{itemize}
 
 \item A weighted graph $W$ is composed of two sets
  \begin{itemize}
   \item A set of vertices $V(W)$ 
   \item A set of edges $E(W)$ such that each edge is a pair of vertices $v_1$ and $v_2$ and a numeric weight $w$.
   \item Weighted graphs are often used in path-finding problems. 
  \end{itemize}
  
  \item Graph representations
   \begin{itemize}
    \item Adjacency list
     \begin{itemize}
      \item It is the simplest and most compact way to represent a graph
      \item Given a graph $G$ such that $V(G) = \{ v_1, v_2, \cdots, v_n \}$, the adjacency list representation of $G$ is a list of length $n$ such that the $i^{th}$ element of the list is a list that contains one element for each vertex adjacent to $v_i$.
     \end{itemize}
     
     \item Adjacency matrix: It is a $n \times b$ matrix and the cell entry is either 1 or 0, indicating whether two vertices are adjacent to each other or not.
     
     \item Incidence matrix: Given a graph $G$ such that $V(G) = \{v-1, v-2, \cdots, v_n \}$ and $E(G) = \{ e_1, e_2, \cdots, e_m \}$, the incidence matrix is an $n \times m$ matrix. Let $a_{r,c}$ represent the matrix value at row $r$ and column $c$. 
     \begin{itemize}
      \item If $G$ is undirected, $a_{r,c} = 1$ if $v_r$ is the head or tail of $e_c$; otherwise $a_{r,c}=0$. 
      \item If $G$ is directed, $a_{r,c} = -1$ if $v_r$ is the tail of $e_c$; $a_{r,c} = 11$ if $v_r$ is the head of $e_c$; $a_{r,c} = 0$ if $e_c$ is a loop; otherwise $a_{r,c}=0$.
     \end{itemize}     
   \end{itemize}
\end{enumerate}

\section{Chapter 4 on May 18, 2015}
\begin{enumerate}
 \item Kernel methods refers to the transformation of data in a problem's input space into a high-dimensional feature space, allowing the algorithms to be performed on the transformed space, i.e., feature space.
 
 \item The implicit transformation can be used by replacing the operations in the analysis algorithm itself with operations corresponding to a different feature space. Noted that the inner product of vectors in feature space can be obtained by using the inner product of the original space. The algorithm can be represented using the inner products and thereby, there is no need to compute the actual feature space at all. This is the kernel trick, and the function used to computes the inner products is the kernel function. Kernel function must be symmetric, i.e., $K(d_i, d_j) = K(d_j, d_i)$ and postive semi-definite. It can be interpreted as a measurement of similarity between pairs of input data. 
  \begin{itemize}
   \item Polynomial $(x \cdot y + \theta)^d$
   \item Gaussian RBF $e^{\frac{|x-y|^2}{c}}$
   \item Sigmoidal $tanh (\alpha(x \cdot y) + \theta)$
  \end{itemize}
\end{enumerate}
 
\section{Chapter 5 on May 19 2015}
\begin{enumerate}
 \item Any network of links can be represented as a graph $G=(V,E)$, where $V$ denotes the set of vertices (nodes) and $E$ denotes the set of edges (links). This abstraction enables us to directly reason about links using concepts from graph theory.
 
 \item Link analysis has several distinct task
  \begin{itemize}
   \item Link-based object classification (LOC) is a technique used to assign class labels to nodes according to their link characteristics. 
   \item Link-based object ranking (LOR) ranks objects in a graph based on several factors affecting their importance in the graph structure, whereas LOC assigns labels specifically belonging to a closed set of finite values to an object. The goal is to associate a relative quantitative assessment with each node. 
   \item Link prediction
  \end{itemize}
 
 \item About social network analysis (SNA)
  \begin{itemize}
   \item Social network analysis (SNA), \emph{sna} package in $R$.
   \item SNA studies the information flow and interactions between the members of a given network. Graphical representation of a social network is a popular way to understand and analyze the behavior of both the individuals and the overall network.
   \item SNA is the representation and analysis of relationships/information flow between individuals, groups, organizations, servers, and other connected entities.
   \item  
  \end{itemize}
  
  \item Metrics for SNA
   \begin{itemize}
    \item Degree: the degress of a vertex is the number of edges incident to it.
    \item Density: the density of a graph is the number of existing edges over the number of possible ones. 
    \item Connectedness: the Krackhardt connectedness for a digraph $G$ is equal to the fraction of all dyads (a group of two nodes), $u$ and $v$, such that there exists an undirected path from $u$ to $v$ in $G$.
    \item Betweenness centrality is a measure of the degree to which a given node lies on the shortest paths between the other nodes in the graph. Note that distance between two nodes in a graph is the minimum number of hops or edge traversals requied to reach the destination node from the starting node. Betweenness is a measure that represents the influence a node has over the connectivity of all the other nodes in a given network. A geodesic is the shortest path between any two nodes in a network. A node has high betweenness if the shortest paths (geodesics) between many pairs of the other nodes in the graph pass through that node. 
    \item Egocentric network of vertex $v$ in graph $G$ is the subgraph of $G$ consisting of $v$ and its neighbors.
    \item Closeness centrality (CLC) is a measure defined for nodes of a given graph. The higher the value of CLC for a node is, the closer the node is to the other nodes in the graph. For a node $v$, CLC is defined as the ratio of the total number of nodes in the graph minus one to the sum of the shortest distances (geodesics) of the node $v$ to every other node in the graph. CLC represents how reachable the nodes of the graph are from any given node under consideration.
    $$ CLC(v) = \frac{|V|-1}{\sum_{i,v \neq v_i} distance(v,v_i)} $$
   \end{itemize}
   
   \item The PageRank algorithm
    \begin{itemize}
     \item It is an algorithm that addresses the linl-based object ranking (LOR) problem. The objective is to assign a numerical rank or priority to each web page by exploiting the "link: structure of the web.
     \item The fundamentals of this algorithm are based on the count and pquality of backlinks or inlinks to a web page. A backlink of a page $P_u$ is a citation to $P_u$from another page. The links from that page is called outlinks. 
     \item The computational and mathematical backbone of PageRank is the power method, which computes an eigenvector of a matrix. It is a recursive procedure that can run for days in case of bigger matrices representing the Web. 
     \item PageRank in $R$ is \texttt{page.rank}. 
    \end{itemize}
    
    \item Vertices as authority and hubs
     \begin{itemize}
      \item A vertex is considered an authority if it has many pages that link to it (i.e., has a high indegree).
      \item A vertex is considered an hun if it has many outgoing links (has a high outdegree).
     \end{itemize}
     
     \item Link prediction
      \begin{itemize}
       \item Link prediction is used to predict new links in a graph. 
       \item Some notations
        \begin{itemize}
         \item $G(t_i, t_j)$ represents the snapshot of graph G between time $t_i$ and $t_j$.
         \item $core$ represents the set of vertices with at least a threshold number of edges
         \item $k_{training}$ is the number of edges a vertex in the training set must have in order to be in the core set
         \item $E_{old}$ represents the set of edges in the training set
         \item $E_{new}$ represents the set of edges in the test set 
        \end{itemize}
        \item Link prediction can be performed based on proximity measures. Proximity measures are used to find similarity between a pair of objects. A threshold on the number of edges a vertex needs to be adjacent to (in both training and testing data) is defined so that the prediction process is performed on a subset of graph. The core set contains vertices that are adjacent to 3 or more edges in the graph. Given the training set $G(V,E_{old})$, the aim is to predict new edges among the vertices in core, in the test set. 
        \item The proximity measures assign a numerical value $score(u,v)$ to the edge connecting a pair of nodes $u$ and $v$. A list $L$ is produced by ranking all such pairs in decreasing order of their scores. The score is defined as a negative value of the shortest path length. 
        \item Some example proximity measures 
         \begin{itemize}
          \item Node neighborhood-based methods
           \begin{itemize}
            \item Common neighbors method is a simple measure which take into account the intersection set of the neighbors of the vertices $u$ and $v$. The score will be $score (u,v) = |N(u) \cap N(v)|$.
            \item Jaccard coefficient is a proximity measure based on the node neighborhood principle. $J(A,B) = \frac{|A \cap B|}{|A \cup B|}$.To measure dissimilarity, just subtract $J(A,B)$ from 1. The Jaccard coefficent can be defined for vertices $u$ and $v$ similarly and used for formulating scores $score(u,v) = \frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}$.
            \item Adamic-Adar method computes the similarity between any two vertices using a common feature of the two, namely $z$. It is defined as $\sum_z \frac{1}{log(freq(z))}$, where $freq(z)$ is the frequency of occurrence of the common feature between $u$ and $v$. Hence $score(u,v) = \sum_{z \in N(u) \cap N(v)} \frac{1}{log(|N(z)|)}$.
           \end{itemize}
           
           \item Ensemble of all path-based methodologies
            \begin{itemize}
             \item PageRank
             \item SimRank
            \end{itemize}
            
            \item Higher level methodologies
             \begin{itemize}
              \item Unseen bigrams
              \item Clustering
             \end{itemize}
         \end{itemize}
      \end{itemize}
\end{enumerate}

\section{Chapter 6 on May 19 2015}
\begin{enumerate}
 \item Two algorithms for measuring proximity between vertices in a graph
  \begin{itemize}
   \item Shared nearest neighbor (SNN) defines the similarity between two vertices in terms of the number of neighbors (i.e., directly connected vertices) they have in common.
   \item The Neumann kernel is a generalization of the HITS that models the relatedness of vertices within a graph based on the number of immediate and more distant connections between vertices.
  \end{itemize}
  
  \item The shared nearest neighbor graph: let $V= \{ v_1, v_2, \cdots, v_n \}$ be the set of nodes in a KNN graph G. In the SNN graph derived from$G$, the neighbors of $v_i$ are the nodes $v_j, j \neq i$ such that $v_i$ and $v_j$ have at least $k$ neighbors in common or equivalently, if there exist at least $k$ distinct paths of length two between $v_i$ and $v_j$ n $G$. SNN is used for undirected but possibly edge-weighted graphs.
  
  \item Evaluating relatedness using Neumann Kernels: It can be used for both directed and undirected graphs. 
\end{enumerate}

\section{Chapter 7 on May 19 2015}
\begin{enumerate}
 \item Frequent subgraph mining (FSM) is the process of discovering subgraphs that occur often in a database of other graphs. FSM can be used to highlight interesting structural properties in graph-based data. It is a type of unsupervised learning.
 
 \item Graphs primarily convey structural information. Finding frequent substructures is often a reliable way to classify data that can be expressed as a graph. 

\item All FSM techniques take a set of graphs as input and returns a set of graph that occur frequently in the input. All techniques share a common basic structure as follows
 \begin{itemize}
  \item Candidate generation
  \item Candidate pruning
  \item Support counting
 \end{itemize}
 
\item Pattern growth approach 
 \begin{itemize}
  \item Modern FSM algorithms begin with a single vertex and grow the graph one edge at a time. Each time a new edge is added, a new vertex may need to be added as well. After each extension, the frequency of the new graph is checked.
  \item Efficiency is enhanced by avoiding checking graphs that are isomorphic to graphs which have already been checked.
\end{itemize}  

\item Frequent subgraph mining package in $R$ is \texttt{subgraphMining}. It contains the SUBDUE, gSpan, and SLEUTH algorithms.

\item gSpan (Graph-based Substucture PAtterN)
 \begin{itemize}
  \item It is a depth-first pattern growth algorithm
  \item It features an important innovation that significantly restricts the number of redundant subgraph candidates that must be examined.
  \item It can significantly outperforming its Apriori-based predecessors and the parallel version exhibits good scalability.
  \item Compared to other general pattern growth algorithms, the only major difference comes during the process of candidate generation, which avoids generating subgraphs that are isomorphic to other subgraphs already considered.
  \item It represents graphs with a special encoding unique to gSpan that is called gSpan encoding. 
  \item gSpan begins with a single edge (two vertices) subgraphs. Each subgraph is extended one edge at a time, and the support of the newly generated graphs is measured. If the support is equal or greater than \emph{min support}, that subgraph will be expanded further. Once it has been extended such that its support is less then \emph{min support}, the algorithm back-tracks until it has returned to a more promising subgraph, allowing gSpan to expand graphs in a depth-first manner.
  \item The gSpan encoding: each edge is represented using five tuples, the first and second are the unique indices of starting and ending points, the third and fifth are the labels of vertices, and the fourth is the edge label.
  \item The rightmost expansion: in a depth-first traversal, the starting node is called the root, and the last node is called the rightmost node. When considering depth-first seach tree, the shortest path from the root to the rightmost node is the rightmost path. When expanding a gSpan encoding via rightmost expansion, it is only possible to add a new edge if one of its endpoints lies on the rightmost path.
  \item Lexicographic order
  \item Minimum gSpan encoding (MgSE) is simply the smallest of all the possible gSpan Encoding (the lowest one in the lexicographic ordering). MgSE can be used to decide whether two graphs are isomorphic. Note that the MgSE is always the same for isomorphic graphs but different for non-isomorphic graphs. MgSE is the key to candidate pruning in gSpan.
 \end{itemize}
 
 \item SUBDUE algorithm
  \begin{itemize}
   \item Note that gSpan is an exact algorithm which find the complete set of frequent subgraphs in a database of graphs. It may be time-consuming for large data. In addition, some of the frequent subgraph may not be interesting, e.g., all the frequent 1-vertex subgraph.
   \item The most iwidely used FSM algorithms, which designed for real-world problems, is the SUBDUE. It uses a constrained form of beam search to discover frequent subgraphs. It reports structures based on the amount of compression they provide for the original gragh. If a subgraph allows for a lot of compression, it is assumed to be interesting and therefore deserves to be further explored. SUBDUE is an efficient but sometimes inexact method.
   \item SUBDUE is fast due to the adoption of beam search. Beam search is a best-first version of breadth-first search, so only the best $k$ children are expanded at each level of the search. The rest are ignored. $k$ is called the beam width of the algorithm.
   \item SUBDUE starts with all the frequent single vertex subgraphs and expand them one edge at a time. SUBDUE only explores the best $k$ of these expanded subgraphs.
   \item The other novel feature is that SUBDUE decides which subgraphs are interesting based on the compression they provided. Compression simply means representing data using a smaller number of bits, analogous to file compression techniques. The subgraphs that SUBDUE considers important are the ones that allow it to compress the original set of graphs into fewest number of bits.
   \item Description length of a graph $G$ is denoted as $DL(G)$. The value of $DL(G)$ is the integer number of bits required to represent graph $G$ in some binary format. 
  \end{itemize}
  
  \item SLEUTH algorithm
   \begin{itemize}
    \item Designed for tree-type graphs
    \item Can be used to mine frequent subtrees within a collection of trees
   \end{itemize}
\end{enumerate}

\section{Chapter 8 on May 21 2015}
\begin{enumerate}
 \item Two types for graph data, i.e., within-graph clustering and between-graph clustering.
 
 \item Minimum spanning tree clustering
  \begin{itemize}
   \item Spanning tree is a conncected subgraph with no cycles that includes all vertices in the graph
   \item Minimum spanning tree (MST) is the spanning tree with the minimum possible sum of edge weights, if the edge weights represent distance and maximum possible sum of edge weights, if the edge weights present similarity.
   \item The canonical algorithm to find MST in an undirected weighted graph is Prim's algorithm. In $R$, the Prim algorithm is implemented in package $igraph$.
   \item Clusters can be created by working on the MST obtained. The removal of $k-1$ edges will create $k$ clusters. The removal begins with those with the larger weights. Can be performed in $R$ using $GraphClusterAnalysis: k_clusterSpanningTree$.
  \end{itemize}
  
  \item Note that MST-based clustering defines similarity between vertices based on the high-weight edges and paths between vertex pairs. Removal of possibly large number of edges results in loss of information in the resulting clusters. A more general notion of similarity between vertex pairs can be captured by the $shared nearest neighbor clustering$
   \begin{itemize}
    \item Unlike the weight-based measures, it provides a natural and robust way of preventing noise in the data set (e.g., the removal or addition of edges into the graph) from affecting the clustering results.
    \item The idea is to place a pair of objects into the same cluster, if the number of common neighbors they share is more than some threshold. It can work on both edge-weighted graphs using some proximity measure (e.g. cosine similarity) and unweighted graphs.
    \item The Jarvis-Patrick clustering is a popular within-graph SNN clustering algorithm. In $R$, can be performed using $igraph, RBGL, GraphClusteringAnalysis$ packages.  
   \end{itemize}
   
  \item Between centrality clustering
   \begin{itemize}
    \item Betweenness centrality quantifies the degree to which a vertex occurs on the shortest path between all the other pairs of nodes. The graph can be clustered by removing nodes with high betweenness centrality, under the assumption that such nodes typically separate different clusters in the graph. 
    \item Girvan and Newman developed a clustering algorithm using edge between centrality. The between centrality of all edges is calculated and the one with the highest edge-betweenness centrality is removed. The process is performed iteratively until the highest edge-betweenness centrality in the graph falls below a user-defined threshold. One shortcoming is that the method divides the graph into non-overlapping clusters.
    \item To overcome this limitation, Pinney and Westhead uses the vertex-betweenness centrality in a graph while retaining the vertices that the algorithm partitions based on. It iteratively divides the graph at the vertex with the maximum betweenness centrality. At the end of the algorithm, each removed vertex $v$ is inserteed into all clusters that share at least one edge with $v$.
    \item The algorithm can be performed using $R$ packages $graph, GraphClusterAnalysis, RBGL$.
   \end{itemize}
  
  \item Clustering vertices with kernel k-means
   \begin{itemize}
    \item k-means clustering can be used to cluster the vertices in a graph, given an appropriate definition of vertex similarity through kernel functions.
    \item Implemented in $R$ using packages $igraph, RBGL, GraphClusterAnalysis, graph, kernlab$.
    \item One drawback is that the graph-based k-mean clustering suffers from the poor initial centroid placement and is prone to local minima. One common strategy is to generate several random sets of centroids and pursue the most promising based on a predetermined measure of cluster quality. 
   \end{itemize}
   
  \item How to choose the clustering algorithms
   \begin{itemize}
    \item Complete (e.g., graph-based k-means) or partial clustering (e.g., maximal clique enumeration).
    \item Overlapping (e.g., maximal clique enumeration and vertex betweenness-based centrality algorithms) or exclusive clustering (e.g., k-means).
   \end{itemize}
\end{enumerate}

\section{Chapter 9 on May 21 2015}
\begin{enumerate}
 \item Two types, i.e., classifying multiple graphs and classifying individual vertices within a single graph. Kernel-based classification techniques are mainly adopted as they have been successfully applied in practice and are based on an implicit graph-to-vector mapping, allowing us to use well known numerical-vector-based classification methods.
 \item Graph classification is to classify a number of individual graphs.
 \item Graph classification using direct product kernel
 \item Vertex classification using the regularized Laplacian kernel
\end{enumerate}

\section{Chapter 11 on May 21 2015}
\begin{enumerate}
 \item Graph-based anomaly detection
  \begin{itemize}
   \item "White crow anomaly" is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism.
   \item "In-disguise anomaly" is an observation that is only a minor deviation from the normal pattern.
   \item "Pointwise anomaly" is a point in time at which the observed values are significantly different than the rest of the timestampls in the time series. 
  \end{itemize}
 
 \item Random walk algorithm can be used to find "white crow anomaly" in dataset without any temporal aspect.
 
 \item GBAD algorithm is applied to find "in-disguise anomalies".
  \begin{itemize}
   \item It is an unsupervised method based on the SUBDUE system, which designed to find anomalies due to modification, insertion, or deletion. 
   \item GBAD-MDL uses the MDL approach to find the best substructure in the graph and subsequently searches for all the other substructures that look similar to the best. The best substructure is the one that occurs the most frequently and causes maximum compression in the graph using the MDL principal.
   \item GBAD-P follows the basic principle of MDL by finding the best substructure in the graph. For each occurrence of that substructure, its extension is examined. The extension can be in the form of adding vertex or edge. A probability of occurrence is calculated for each extension and the one with the lower probability is considered anomalous. 
   \item GBAD-MPS also uses the MDL to find the best substructure $S$ in the graph. The parent substructure of $S$ has the exact same structure of $S$ with one or model nodes or edges removed. The algorithm finds the substructure that match the parent substructure of $S$. Cost is associated with addition of vertex or edge. The substructure that occur infrequently and require a higher cost to transform are considered anomalous.
  \end{itemize}
  
 \item Research on "white crow" anomalies is mainly focused on exploring three different anomalies, i.e., anomalous nodes, edges, and subgraphs. 
 
 \item "In-disguise" anomalies are more difficult to detect. The GBAD methods are designed to find such anomalies which may include label modification, vertex or edge insertion, and vertex or edge deletion. Note that they are not designed to find anomalies in multiple graphs.
\end{enumerate}

\section{Chapter 12 on May 21 2015}
\begin{enumerate}
 \item Supervised learning performance matrics
  \begin{itemize}
   \item Confusion matrix for classification problems
    \begin{itemize}
     \item $accuracy = \frac{TP+TN}{TP+TN+FP+FN}$
     \item $accuracy = \frac{FP+FN}{TP+TN+FP+FN}$
     \item Recall quantifies the notion of sensitivity. It contains positive recall (true positive rate) and negative recall (true negative rate).  $$TPR = \frac{TP}{TP+FP}$$ $$NR=\frac{TN}{TN+FN}$$
     \item Misclassification rate is analogous to recall and has false positive rate and false negative rate. 
     $$FPR = \frac{FP}{TN+FP}$$ $$TNR=\frac{FN}{TP+FN}$$
     \item Precision quantifies the notion of specificity. $P^+ = \frac{TP}{TP+FN}$ or $P^- = \frac{TN}{TN+FP}$
     \item $F-measure$ is the harmonic mean of precision and recall values of a class and provides a way to judge the overall performance of the model for each class. 
     $$F^+ = \frac{2 \times P^+ \times TPR}{P^+ + TPR}$$
     $$F^- = \frac{2 \times P^- \times TNR}{P^- + TNR}$$
     \item $G-mean$ is the geometric mean of TPR and TNR (positive and negative recalls): $G-mean = \sqrt[2]{TPR \times TNR}$. This one is a better way to quantify the performance of the model since it is less prone to the effects of the underlying class distribution. It works especially well for imbalanced classification problems.
     \item Critical success index (CSI) is also called the threat score, is the proportion of the correct predictions of class $L$ of the sum of all predicted values of $L$ and all values of $L$ not correctly predicted. It is useful for multi-class classification problems.
     \item Hit rate (HR) is the success rate of each class.
     \item Bias is the ratio of the total points with label $L$ to the number of points predicted as $L$. If $> 1$ means under-prediction and $<1$ means over-prediction for a certain class $L$.
    \end{itemize}
  \end{itemize}
 
 \item Unsupervised learning performance metrics
  \begin{itemize}
   \item Evaluation using prior knowledge
    \begin{itemize}
     \item Contingency table
     \item Rand statistic is also called the simple matching coefficient. It is a measure where both placing a pair of points with the same class label in the same cluster and placing a pair of points with different class labels in different clusters are given equal importance, i.e., it accounts for both specificity and sensitivity of clustering.
     \item Jaccard coefficient is used when placing a pair of points with the same class label in the same cluster is primarily important.
     \item Entropy, 0 means homogeneous and 1 means impurity.
     \item For clusters
      \begin{itemize}
       \item Cohesion and separation
       \item Dunn index ranges from 0 to infinity and should be maximized.
       \item Silhouette coefficient
      \end{itemize}
    \end{itemize}    
  \end{itemize}
\end{enumerate}

\end{document}